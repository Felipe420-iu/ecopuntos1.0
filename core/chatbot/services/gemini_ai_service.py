"""
Servicio de IA usando Google Gemini 1.5 Flash

CaracterÃ­sticas principales:
- GRATIS hasta 1,500 requests/dÃ­a
- 1 millÃ³n de tokens de contexto
- Multimodal nativo (texto, imÃ¡genes, audio)
- Latencia mÃ¡s baja
- API mÃ¡s simple
"""

import os
import logging
import asyncio
from typing import Dict, List, Optional, Tuple
from datetime import datetime

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logging.warning("google-generativeai no estÃ¡ instalado. Ejecuta: pip install google-generativeai")

from django.conf import settings
from core.models import MensajeChatbot, ContextoChatbot

logger = logging.getLogger(__name__)


class GeminiAIService:
    """Servicio de IA usando Gemini Flash 2.0 - Gratis y Potente"""
    
    def __init__(self):
        """Inicializa el servicio de Gemini"""
        if not GEMINI_AVAILABLE:
            raise ImportError("Instala google-generativeai: pip install google-generativeai")
        
        # Configurar API key
        api_key = getattr(settings, 'GOOGLE_API_KEY', os.getenv('GOOGLE_API_KEY'))
        if not api_key:
            raise ValueError("GOOGLE_API_KEY no configurada en settings o .env")
        
        genai.configure(api_key=api_key)
        
        # Modelo: gemini-2.5-flash (versiÃ³n mÃ¡s reciente y gratuita)
        self.model_name = getattr(settings, 'AI_MODEL', 'gemini-2.5-flash')
        
        # ConfiguraciÃ³n de generaciÃ³n
        self.generation_config = {
            'temperature': 0.7,  # Equilibrio entre creatividad y precisiÃ³n
            'top_p': 0.95,
            'top_k': 40,
            'max_output_tokens': 1024,  # Suficiente para respuestas del chatbot
        }
        
        # Configuraciones de seguridad (ajustables segÃºn necesidad)
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
        ]
        
        # Crear modelo
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=self.generation_config,
            safety_settings=self.safety_settings
        )
        
        logger.info(f"âœ… Gemini AI Service inicializado con {self.model_name}")
    
    def _build_system_prompt(self, user) -> str:
        """
        Construye el prompt del sistema con informaciÃ³n contextual
        
        VENTAJA GEMINI: Con 1M de tokens, podemos incluir MUCHA informaciÃ³n
        sin preocuparnos por el lÃ­mite
        """
        return f"""Eres EcoBot, el asistente inteligente oficial de EcoPuntos, una plataforma de reciclaje gamificada en Colombia.

INFORMACIÃ“N DEL USUARIO ACTUAL:
- Nombre: {user.get_full_name() or user.username}
- Puntos Actuales: {getattr(user, 'puntos', 0)} EcoPuntos
- Nivel: {getattr(user, 'level', 'GuardiÃ¡n Verde')}

INFORMACIÃ“N CORRECTA SOBRE ECOPUNTOS:
=======================================

Â¿QUÃ‰ ES ECOPUNTOS?
EcoPuntos es una plataforma donde los usuarios entregan materiales reciclables y obtienen puntos que pueden canjear por recompensas especÃ­ficas (NO por dinero).

SISTEMA DE FUNCIONAMIENTO REAL:
1. **ENTREGA DE MATERIALES**: Los usuarios solicitan canjes entregando materiales reciclables
2. **VERIFICACIÃ“N**: El equipo de EcoPuntos verifica y pesa los materiales
3. **PUNTOS**: Se otorgan puntos segÃºn el peso real y tipo de material
4. **RECOMPENSAS**: Los puntos se canjean Ãºnicamente por recompensas del catÃ¡logo

MATERIALES ACEPTADOS Y PUNTOS:
- **PlÃ¡sticos**: Puntos variables por kilogramo segÃºn tipo
- **Vidrio**: Puntos especÃ­ficos por kilogramo 
- **Papel y CartÃ³n**: Puntos especÃ­ficos por kilogramo
- **Metales**: Puntos especÃ­ficos por kilogramo
(Los puntos exactos por kilo se definen por material especÃ­fico)

PROCESO DE CANJE:
1. Usuario solicita canje especificando material y peso estimado
2. Equipo programa recolecciÃ³n o usuario entrega en punto de acopio
3. Material se verifica y pesa (peso real puede diferir del estimado)
4. Se aprueban puntos segÃºn peso real Ã— puntos por kilo del material
5. Puntos se agregan automÃ¡ticamente a la cuenta del usuario

RECOMPENSAS DISPONIBLES:
Los puntos SOLO se pueden canjear por:
- Productos del catÃ¡logo de recompensas
- Experiencias especÃ­ficas disponibles
- Beneficios y descuentos con aliados
- NUNCA por dinero en efectivo

NIVELES DE USUARIO:
- GuardiÃ¡n Verde (inicial)
- Defensor del Planeta  
- HÃ©roe Eco
- Embajador Ambiental
- Leyenda Sustentable

FUNCIONALIDADES ADICIONALES:
- Juegos educativos de reciclaje (se pueden ganar puntos jugando)
- Sistema de notificaciones
- Rutas de recolecciÃ³n programadas
- Seguimiento de estadÃ­sticas personales

ESTADOS DE CANJES:
- Pendiente: Solicitud enviada
- Confirmado: Solicitud aceptada
- En RecolecciÃ³n: Material siendo recolectado
- Verificando: Material siendo verificado
- Aprobado: Puntos otorgados
- Rechazado: Material no aceptado
- Completado: Proceso finalizado

TU PERSONALIDAD Y TONO:
- Amigable, motivador y entusiasta sobre el reciclaje
- Educativo pero no condescendiente  
- Usa emojis ocasionalmente â™»ï¸ ðŸŒ± ðŸŽ¯
- Celebra los logros del usuario
- Motiva a seguir reciclando

RESTRICCIONES IMPORTANTES:
âŒ NUNCA digas que los puntos se pueden canjear por dinero
âŒ NUNCA inventes informaciÃ³n sobre recompensas que no existen
âŒ NUNCA prometas funciones que no estÃ¡n implementadas
âŒ NUNCA des informaciÃ³n incorrecta sobre el proceso de canjes

âœ… SIEMPRE explica que los puntos son para recompensas especÃ­ficas
âœ… SIEMPRE menciona que los materiales deben ser verificados
âœ… SIEMPRE deriva a soporte si no sabes algo especÃ­fico
âœ… SIEMPRE celebra cuando el usuario recicla

FRASES CLAVE A USAR:
- "Los puntos se canjean por recompensas del catÃ¡logo, no por dinero"
- "El equipo verificarÃ¡ tus materiales y asignarÃ¡ puntos segÃºn el peso real"
- "Â¡Excelente contribuciÃ³n al medio ambiente!"
- "Cada material reciclado cuenta para un planeta mÃ¡s limpio"

EJEMPLO DE RESPUESTA CORRECTA:
Usuario: "Â¿Puedo sacar dinero de mis puntos?"
Respuesta: "Los EcoPuntos no se pueden convertir en dinero ðŸ’°. En lugar de eso, puedes canjearlos por increÃ­bles recompensas de nuestro catÃ¡logo: productos ecolÃ³gicos, experiencias, descuentos con aliados y mucho mÃ¡s. Â¡Es una forma genial de obtener beneficios mientras cuidas el planeta! ðŸŒ± Â¿Te gustarÃ­a que te muestre quÃ© recompensas estÃ¡n disponibles?"

Recuerda: Tu objetivo es hacer que reciclar sea fÃ¡cil, divertido y gratificante, siempre con informaciÃ³n precisa."""
    
    async def _build_conversation_history(
        self, 
        conversacion_id: int, 
        include_context: bool = True,
        max_messages: int = 20  # Puedes aumentar esto gracias al contexto de 1M
    ) -> List[Dict[str, str]]:
        """
        Construye el historial de conversaciÃ³n (versiÃ³n async)
        
        VENTAJA GEMINI: Podemos incluir muchos mÃ¡s mensajes sin problema
        """
        from channels.db import database_sync_to_async
        
        messages = []
        
        try:
            # Obtener mensajes recientes (async)
            @database_sync_to_async
            def get_mensajes():
                mensajes = MensajeChatbot.objects.filter(
                    conversacion_id=conversacion_id
                ).order_by('-timestamp')[:max_messages]
                return list(reversed(mensajes))
            
            mensajes = await get_mensajes()
            
            # Construir historial
            for mensaje in mensajes:
                role = 'user' if mensaje.es_usuario else 'model'
                messages.append({
                    'role': role,
                    'parts': [mensaje.contenido]
                })
            
            logger.info(f"ðŸ“š Historial construido: {len(messages)} mensajes")
            
        except Exception as e:
            logger.error(f"Error al construir historial: {e}")
        
        return messages
    
    async def process_message(
        self,
        user,
        mensaje: str,
        conversacion_id: int,
        include_history: bool = True
    ) -> Tuple[str, float]:
        """
        Procesa un mensaje del usuario y genera respuesta con Gemini
        
        Args:
            user: Usuario de Django
            mensaje: Texto del mensaje
            conversacion_id: ID de la conversaciÃ³n
            include_history: Si incluir historial de chat
        
        Returns:
            Tupla (respuesta, confidence)
        """
        try:
            # Construir sistema de prompts
            system_prompt = self._build_system_prompt(user)
            
            # Iniciar chat session
            if include_history:
                history = await self._build_conversation_history(conversacion_id)
            else:
                history = []
            
            # Gemini usa sessions para mantener contexto
            chat = self.model.start_chat(history=history)
            
            # Construir mensaje completo
            if not history:  # Primera interacciÃ³n
                full_message = f"{system_prompt}\n\nUsuario: {mensaje}"
            else:
                full_message = mensaje
            
            # Generar respuesta (sÃ­ncrono - Gemini no tiene async nativo)
            # Ejecutar en thread pool para no bloquear
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: chat.send_message(full_message)
            )
            
            # Extraer texto de respuesta
            respuesta = response.text
            
            # Calcular confidence (Gemini no proporciona directamente, usamos heurÃ­stica)
            confidence = self._calculate_confidence(response)
            
            logger.info(f"âœ… Respuesta generada - Confidence: {confidence:.2f}")
            logger.debug(f"Respuesta: {respuesta[:100]}...")
            
            return respuesta, confidence
            
        except Exception as e:
            logger.error(f"âŒ Error en process_message: {e}")
            return self._get_fallback_response(), 0.5
    
    def _calculate_confidence(self, response) -> float:
        """
        Calcula nivel de confianza de la respuesta
        
        Gemini no proporciona confidence score directamente,
        asÃ­ que usamos heurÃ­sticas
        """
        try:
            # Si la respuesta fue bloqueada por seguridad
            if hasattr(response, 'prompt_feedback'):
                if response.prompt_feedback.block_reason:
                    return 0.0
            
            # Longitud de respuesta como indicador
            text_length = len(response.text)
            if text_length < 20:
                return 0.3  # Respuesta muy corta, baja confianza
            elif text_length < 50:
                return 0.5
            elif text_length < 100:
                return 0.7
            else:
                return 0.9  # Respuesta completa
            
        except Exception as e:
            logger.warning(f"No se pudo calcular confidence: {e}")
            return 0.7  # Default medio
    
    def _get_fallback_response(self) -> str:
        """Respuesta de respaldo si algo falla"""
        return (
            "Disculpa, estoy teniendo problemas tÃ©cnicos en este momento. ðŸ”§\n\n"
            "Por favor:\n"
            "- Intenta reformular tu pregunta\n"
            "- O contacta a soporte: soporte@ecopuntos.com\n\n"
            "Â¡Gracias por tu paciencia! â™»ï¸"
        )
    
    async def detect_intent(self, mensaje: str) -> Dict[str, any]:
        """
        Detecta la intenciÃ³n del mensaje usando Gemini
        
        VENTAJA: DetecciÃ³n mÃ¡s precisa sin necesidad de fine-tuning
        """
        try:
            prompt = f"""Analiza este mensaje de un usuario y clasifica su intenciÃ³n:

Mensaje: "{mensaje}"

Clasifica en UNA de estas categorÃ­as:
1. consulta_reciclaje - Pregunta sobre quÃ©/cÃ³mo reciclar
2. consulta_puntos - Pregunta sobre puntos ganados o disponibles
3. consulta_recompensas - Pregunta sobre canjeo o recompensas
4. solicitud_recogida - Quiere agendar recogida de materiales
5. problema_tecnico - Reporta un error o problema
6. saludo - Saludo o despedida
7. otro - No encaja en las anteriores

Responde SOLO con el formato JSON:
{{"intent": "categoria", "confidence": 0.0-1.0, "entities": []}}"""

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content(prompt)
            )
            
            # Parsear respuesta JSON
            import json
            result = json.loads(response.text.strip())
            
            logger.info(f"ðŸŽ¯ IntenciÃ³n detectada: {result['intent']} ({result['confidence']:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"Error en detect_intent: {e}")
            return {
                'intent': 'otro',
                'confidence': 0.5,
                'entities': []
            }
    
    async def analyze_image(self, image_path: str, question: str = None) -> str:
        """
        Analiza una imagen usando Gemini (MULTIMODAL)
        
        VENTAJA EXCLUSIVA: Gemini Flash es multimodal nativo
        Casos de uso:
        - Usuario envÃ­a foto de material: "Â¿Es reciclable?"
        - VerificaciÃ³n de estado de materiales
        - ClasificaciÃ³n automÃ¡tica
        """
        try:
            # Cargar imagen
            import PIL.Image
            img = PIL.Image.open(image_path)
            
            # Prompt para anÃ¡lisis
            if question:
                prompt = question
            else:
                prompt = """Analiza esta imagen de material/producto:

1. Â¿QuÃ© material es?
2. Â¿Es reciclable en EcoPuntos?
3. Â¿A quÃ© categorÃ­a pertenece? (papel, plÃ¡stico, vidrio, metal, electrÃ³nico)
4. Â¿CuÃ¡ntos puntos podrÃ­a ganar?
5. Â¿Alguna recomendaciÃ³n para prepararlo para reciclaje?

Responde de forma clara y amigable."""
            
            # Generar respuesta con imagen
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content([prompt, img])
            )
            
            logger.info("ðŸ“¸ Imagen analizada exitosamente")
            return response.text
            
        except Exception as e:
            logger.error(f"Error en analyze_image: {e}")
            return "No pude analizar la imagen. Por favor intenta de nuevo."
    
    def get_model_info(self) -> Dict[str, any]:
        """InformaciÃ³n del modelo actual"""
        return {
            'provider': 'Google Gemini',
            'model': self.model_name,
            'version': '1.5 Flash',
            'context_window': '1,000,000 tokens',
            'cost': 'GRATIS hasta 1,500 requests/dÃ­a',
            'multimodal': True,
            'features': [
                'Contexto de 1M tokens',
                'AnÃ¡lisis de imÃ¡genes',
                'Baja latencia',
                'API simple'
            ]
        }


# FunciÃ³n helper para fÃ¡cil integraciÃ³n
async def get_gemini_response(user, mensaje: str, conversacion_id: int) -> Tuple[str, float]:
    """
    FunciÃ³n helper para obtener respuesta de Gemini
    
    Uso:
        respuesta, confidence = await get_gemini_response(user, "Hola", 1)
    """
    service = GeminiAIService()
    return await service.process_message(user, mensaje, conversacion_id)


# Testing rÃ¡pido
if __name__ == "__main__":
    async def test():
        """Test rÃ¡pido del servicio"""
        print("ðŸ§ª Testing Gemini AI Service...")
        
        # Crear servicio
        service = GeminiAIService()
        print(f"âœ… Servicio creado: {service.model_name}")
        
        # Info del modelo
        info = service.get_model_info()
        print(f"\nðŸ“Š InformaciÃ³n del modelo:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        
        print("\nâœ… Test completado!")
    
    # Ejecutar test
    asyncio.run(test())
